{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from scipy import misc\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from load_dataset import load_test_data, load_batch\n",
    "from ssim import MultiScaleSSIM\n",
    "import models\n",
    "import utils\n",
    "import vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining size of the training image patches\n",
    "\n",
    "PATCH_WIDTH = 100\n",
    "PATCH_HEIGHT = 100\n",
    "PATCH_SIZE = PATCH_WIDTH * PATCH_HEIGHT * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# processing command arguments\\n\\nphone, batch_size, train_size, learning_rate, num_train_iters, w_content, w_color, w_texture, w_tv, dped_dir, vgg_dir, eval_step = utils.process_command_args(sys.argv)\\n\\nnp.random.seed(0)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# processing command arguments\n",
    "\n",
    "phone, batch_size, train_size, learning_rate, num_train_iters, \\\n",
    "w_content, w_color, w_texture, w_tv, \\\n",
    "dped_dir, vgg_dir, eval_step = utils.process_command_args(sys.argv)\n",
    "\n",
    "np.random.seed(0)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=50\n",
    "dped_dir='dped/'\n",
    "w_color=0.7\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'phone' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-8c256db3807d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loading test data...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_answ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_test_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdped_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Test data was loaded\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'phone' is not defined"
     ]
    }
   ],
   "source": [
    "# loading training and test data\n",
    "\n",
    "print(\"Loading test data...\")\n",
    "test_data, test_answ = load_test_data(phone, dped_dir, PATCH_SIZE)\n",
    "print(\"Test data was loaded\\n\")\n",
    "\n",
    "print(\"Loading training data...\")\n",
    "train_data, train_answ = load_batch(phone, dped_dir, train_size, PATCH_SIZE)\n",
    "print(\"Training data was loaded\\n\")\n",
    "\n",
    "TEST_SIZE = test_data.shape[0]\n",
    "num_test_batches = int(test_data.shape[0]/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TensorFlow Definitions:\n",
    "\n",
    "1. tf.Graph(): A TensorFlow computation, represented as a dataflow graph.\n",
    "\n",
    "2. tf.Graph().as_default(): \n",
    "\n",
    "    Returns a context manager that makes this `Graph` the default graph.\n",
    "\n",
    "    This method should be used if you want to create multiple graphs\n",
    "    in the same process. For convenience, a global default graph is\n",
    "    provided, and all ops will be added to this graph if you do not\n",
    "    create a new graph explicitly. Use this method with the `with` keyword\n",
    "    to specify that ops created within the scope of a block should be\n",
    "    added to this graph.\n",
    "\n",
    "    The default graph is a property of the current thread. If you\n",
    "    create a new thread, and wish to use the default graph in that\n",
    "    thread, you must explicitly add a `with g.as_default():` in that\n",
    "    thread's function.\n",
    "\n",
    "3. tf.Session(): \n",
    "    A class for running TensorFlow operations.\n",
    "\n",
    "    A `Session` object encapsulates the environment in which `Operation`\n",
    "    objects are executed, and `Tensor` objects are evaluated. For\n",
    "\n",
    "4. tf.placeholder(dtype, shape=None, name=None):\n",
    "    Inserts a placeholder for a tensor that will be always fed.\n",
    "    Example.,\n",
    "        x = tf.placeholder(tf.float32, shape=(1024, 1024))\n",
    "        y = tf.matmul(x, x)\n",
    "\n",
    "5. tf.reshape(tensor, shape, name=None):\n",
    "    Reshapes a tensor.\n",
    "\n",
    "6. tf.multiply(x, y, name= None):\n",
    "    Returns x * y element-wise.\n",
    "    \n",
    "7. tf.concat(values, axis, name='concat'):\n",
    "    Concatenates tensors along one dimension.\n",
    "    \n",
    "8. tf.reduce_sum(\n",
    "    input_tensor,\n",
    "    axis=None,\n",
    "    keep_dims=False,\n",
    "    name=None,\n",
    "    reduction_indices=None,\n",
    "    )\n",
    "    Computes the sum of elements across dimensions of a tensor.\n",
    "    \n",
    "\n",
    "9. tf.equal(x, y, name=None)\n",
    "    Returns the truth value of (x == y) element-wise.\n",
    "    \n",
    "10. tf.cast(x, dtype, name=None)\n",
    "    Casts a tensor to a new type.\n",
    "\n",
    "    The operation casts `x` (in case of `Tensor`) or `x.values`\n",
    "    (in case of `SparseTensor`) to `dtype`.\n",
    "    \n",
    "11. Signature:\n",
    "    tf.reduce_mean(\n",
    "        input_tensor,\n",
    "        axis=None,\n",
    "        keep_dims=False,\n",
    "        name=None,\n",
    "        reduction_indices=None,\n",
    "    )\n",
    "    Computes the mean of elements across dimensions of a tensor.\n",
    "\n",
    "12. tf.nn.l2_loss(t, name=None)\n",
    "    L2 Loss.\n",
    "\n",
    "    Computes half the L2 norm of a tensor without the `sqrt`:\n",
    "\n",
    "        output = sum(t ** 2) / 2\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vgg_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-72a97006dc07>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0mCONTENT_LAYER\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'relu5_4'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0menhanced_vgg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvgg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvgg_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvgg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menhanced\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m     \u001b[0mdslr_vgg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvgg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvgg_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvgg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdslr_image\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vgg_dir' is not defined"
     ]
    }
   ],
   "source": [
    "# defining system architecture\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    \n",
    "    # placeholders for training data\n",
    "\n",
    "    phone_ = tf.placeholder(tf.float32, [None, PATCH_SIZE])\n",
    "    phone_image = tf.reshape(phone_, [-1, PATCH_HEIGHT, PATCH_WIDTH, 3])\n",
    "\n",
    "    dslr_ = tf.placeholder(tf.float32, [None, PATCH_SIZE])\n",
    "    dslr_image = tf.reshape(dslr_, [-1, PATCH_HEIGHT, PATCH_WIDTH, 3])\n",
    "\n",
    "    adv_ = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "    # get processed enhanced image\n",
    "\n",
    "    enhanced = models.resnet(phone_image)\n",
    "\n",
    "    # transform both dslr and enhanced images to grayscale\n",
    "\n",
    "    enhanced_gray = tf.reshape(tf.image.rgb_to_grayscale(enhanced), [-1, PATCH_WIDTH * PATCH_HEIGHT])\n",
    "    dslr_gray = tf.reshape(tf.image.rgb_to_grayscale(dslr_image),[-1, PATCH_WIDTH * PATCH_HEIGHT])\n",
    "\n",
    "    # push randomly the enhanced or dslr image to an adversarial CNN-discriminator\n",
    "\n",
    "    adversarial_ = tf.multiply(enhanced_gray, 1 - adv_) + tf.multiply(dslr_gray, adv_)\n",
    "    adversarial_image = tf.reshape(adversarial_, [-1, PATCH_HEIGHT, PATCH_WIDTH, 1])\n",
    "\n",
    "    discrim_predictions = models.adversarial(adversarial_image)\n",
    "\n",
    "    # losses\n",
    "    # 1) texture (adversarial) loss\n",
    "\n",
    "    discrim_target = tf.concat([adv_, 1 - adv_], 1)\n",
    "\n",
    "    loss_discrim = -tf.reduce_sum(discrim_target * tf.log(tf.clip_by_value(discrim_predictions, 1e-10, 1.0)))\n",
    "    loss_texture = -loss_discrim\n",
    "\n",
    "    correct_predictions = tf.equal(tf.argmax(discrim_predictions, 1), tf.argmax(discrim_target, 1))\n",
    "    discim_accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
    "\n",
    "    # 2) content loss\n",
    "\n",
    "    CONTENT_LAYER = 'relu5_4'\n",
    "\n",
    "    enhanced_vgg = vgg.net(vgg_dir, vgg.preprocess(enhanced * 255))\n",
    "    dslr_vgg = vgg.net(vgg_dir, vgg.preprocess(dslr_image * 255))\n",
    "\n",
    "    content_size = utils._tensor_size(dslr_vgg[CONTENT_LAYER]) * batch_size\n",
    "    loss_content = 2 * tf.nn.l2_loss(enhanced_vgg[CONTENT_LAYER] - dslr_vgg[CONTENT_LAYER]) / content_size\n",
    "\n",
    "    # 3) color loss\n",
    "\n",
    "    enhanced_blur = utils.blur(enhanced)\n",
    "    dslr_blur = utils.blur(dslr_image)\n",
    "\n",
    "    loss_color = tf.reduce_sum(tf.pow(dslr_blur - enhanced_blur, 2))/(2 * batch_size)\n",
    "\n",
    "    # 4) total variation loss\n",
    "\n",
    "    batch_shape = (batch_size, PATCH_WIDTH, PATCH_HEIGHT, 3)\n",
    "    tv_y_size = utils._tensor_size(enhanced[:,1:,:,:])\n",
    "    tv_x_size = utils._tensor_size(enhanced[:,:,1:,:])\n",
    "    y_tv = tf.nn.l2_loss(enhanced[:,1:,:,:] - enhanced[:,:batch_shape[1]-1,:,:])\n",
    "    x_tv = tf.nn.l2_loss(enhanced[:,:,1:,:] - enhanced[:,:,:batch_shape[2]-1,:])\n",
    "    loss_tv = 2 * (x_tv/tv_x_size + y_tv/tv_y_size) / batch_size\n",
    "\n",
    "    # final loss\n",
    "\n",
    "    loss_generator = w_content * loss_content + w_texture * loss_texture + w_color * loss_color + w_tv * loss_tv\n",
    "\n",
    "    # psnr loss\n",
    "\n",
    "    enhanced_flat = tf.reshape(enhanced, [-1, PATCH_SIZE])\n",
    "\n",
    "    loss_mse = tf.reduce_sum(tf.pow(dslr_ - enhanced_flat, 2))/(PATCH_SIZE * batch_size)\n",
    "    loss_psnr = 20 * utils.log10(1.0 / tf.sqrt(loss_mse))\n",
    "\n",
    "    # optimize parameters of image enhancement (generator) and discriminator networks\n",
    "\n",
    "    generator_vars = [v for v in tf.global_variables() if v.name.startswith(\"generator\")]\n",
    "    discriminator_vars = [v for v in tf.global_variables() if v.name.startswith(\"discriminator\")]\n",
    "\n",
    "    train_step_gen = tf.train.AdamOptimizer(learning_rate).minimize(loss_generator, var_list=generator_vars)\n",
    "    train_step_disc = tf.train.AdamOptimizer(learning_rate).minimize(loss_discrim, var_list=discriminator_vars)\n",
    "\n",
    "    saver = tf.train.Saver(var_list=generator_vars, max_to_keep=100)\n",
    "\n",
    "    print('Initializing variables')\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    print('Training network')\n",
    "\n",
    "    train_loss_gen = 0.0\n",
    "    train_acc_discrim = 0.0\n",
    "\n",
    "    all_zeros = np.reshape(np.zeros((batch_size, 1)), [batch_size, 1])\n",
    "    test_crops = test_data[np.random.randint(0, TEST_SIZE, 5), :]\n",
    "\n",
    "    logs = open('models/' + phone + '.txt', \"w+\")\n",
    "    logs.close()\n",
    "\n",
    "    for i in range(num_train_iters):\n",
    "\n",
    "        # train generator\n",
    "\n",
    "        idx_train = np.random.randint(0, train_size, batch_size)\n",
    "\n",
    "        phone_images = train_data[idx_train]\n",
    "        dslr_images = train_answ[idx_train]\n",
    "\n",
    "        [loss_temp, temp] = sess.run([loss_generator, train_step_gen],\n",
    "                                        feed_dict={phone_: phone_images, dslr_: dslr_images, adv_: all_zeros})\n",
    "        train_loss_gen += loss_temp / eval_step\n",
    "\n",
    "        # train discriminator\n",
    "\n",
    "        idx_train = np.random.randint(0, train_size, batch_size)\n",
    "\n",
    "        # generate image swaps (dslr or enhanced) for discriminator\n",
    "        swaps = np.reshape(np.random.randint(0, 2, batch_size), [batch_size, 1])\n",
    "\n",
    "        phone_images = train_data[idx_train]\n",
    "        dslr_images = train_answ[idx_train]\n",
    "\n",
    "        [accuracy_temp, temp] = sess.run([discim_accuracy, train_step_disc],\n",
    "                                        feed_dict={phone_: phone_images, dslr_: dslr_images, adv_: swaps})\n",
    "        train_acc_discrim += accuracy_temp / eval_step\n",
    "\n",
    "        if i % eval_step == 0:\n",
    "\n",
    "            # test generator and discriminator CNNs\n",
    "\n",
    "            test_losses_gen = np.zeros((1, 6))\n",
    "            test_accuracy_disc = 0.0\n",
    "            loss_ssim = 0.0\n",
    "\n",
    "            for j in range(num_test_batches):\n",
    "\n",
    "                be = j * batch_size\n",
    "                en = (j+1) * batch_size\n",
    "\n",
    "                swaps = np.reshape(np.random.randint(0, 2, batch_size), [batch_size, 1])\n",
    "\n",
    "                phone_images = test_data[be:en]\n",
    "                dslr_images = test_answ[be:en]\n",
    "\n",
    "                [enhanced_crops, accuracy_disc, losses] = sess.run([enhanced, discim_accuracy, \\\n",
    "                                [loss_generator, loss_content, loss_color, loss_texture, loss_tv, loss_psnr]], \\\n",
    "                                feed_dict={phone_: phone_images, dslr_: dslr_images, adv_: swaps})\n",
    "\n",
    "                test_losses_gen += np.asarray(losses) / num_test_batches\n",
    "                test_accuracy_disc += accuracy_disc / num_test_batches\n",
    "\n",
    "                loss_ssim += MultiScaleSSIM(np.reshape(dslr_images * 255, [batch_size, PATCH_HEIGHT, PATCH_WIDTH, 3]),\n",
    "                                                    enhanced_crops * 255) / num_test_batches\n",
    "\n",
    "            logs_disc = \"step %d, %s | discriminator accuracy | train: %.4g, test: %.4g\" % \\\n",
    "                  (i, phone, train_acc_discrim, test_accuracy_disc)\n",
    "\n",
    "            logs_gen = \"generator losses | train: %.4g, test: %.4g | content: %.4g, color: %.4g, texture: %.4g, tv: %.4g | psnr: %.4g, ssim: %.4g\\n\" % \\\n",
    "                  (train_loss_gen, test_losses_gen[0][0], test_losses_gen[0][1], test_losses_gen[0][2],\n",
    "                   test_losses_gen[0][3], test_losses_gen[0][4], test_losses_gen[0][5], loss_ssim)\n",
    "\n",
    "            print(logs_disc)\n",
    "            print(logs_gen)\n",
    "\n",
    "            # save the results to log file\n",
    "\n",
    "            logs = open('models/' + phone + '.txt', \"a\")\n",
    "            logs.write(logs_disc)\n",
    "            logs.write('\\n')\n",
    "            logs.write(logs_gen)\n",
    "            logs.write('\\n')\n",
    "            logs.close()\n",
    "\n",
    "            # save visual results for several test image crops\n",
    "\n",
    "            enhanced_crops = sess.run(enhanced, feed_dict={phone_: test_crops, dslr_: dslr_images, adv_: all_zeros})\n",
    "\n",
    "            idx = 0\n",
    "            for crop in enhanced_crops:\n",
    "                before_after = np.hstack((np.reshape(test_crops[idx], [PATCH_HEIGHT, PATCH_WIDTH, 3]), crop))\n",
    "                misc.imsave('results/' + str(phone)+ \"_\" + str(idx) + '_iteration_' + str(i) + '.jpg', before_after)\n",
    "                idx += 1\n",
    "\n",
    "            train_loss_gen = 0.0\n",
    "            train_acc_discrim = 0.0\n",
    "\n",
    "            # save the model that corresponds to the current iteration\n",
    "\n",
    "            saver.save(sess, 'models/' + str(phone) + '_iteration_' + str(i) + '.ckpt', write_meta_graph=False)\n",
    "\n",
    "            # reload a different batch of training data\n",
    "\n",
    "            del train_data\n",
    "            del train_answ\n",
    "            train_data, train_answ = load_batch(phone, dped_dir, train_size, PATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
